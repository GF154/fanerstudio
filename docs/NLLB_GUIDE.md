# üá≠üáπ Guide NLLB - Tradiksyon Krey√≤l Ayisyen

## Ki sa NLLB ye?

NLLB (No Language Left Behind) se yon mod√®l tradiksyon ki f√®t pa Meta/Facebook pou sip√≤te 200+ lang, espesyalman lang ki pa gen anpil resous tankou Krey√≤l Ayisyen.

## Avantaj NLLB

### ‚úÖ Pou Krey√≤l Ayisyen

| Karakteristik | Google Translate | NLLB |
|---------------|------------------|------|
| **Kalite tradiksyon** | Byen | ‚≠ê **Ekselan** |
| **Sip√≤ Krey√≤l** | Limite | ‚≠ê Natif (hat_Latn) |
| **Vit√®s** | Rapid | Mwayen |
| **Bezwen Ent√®n√®t** | Wi | ‚≠ê Non (lokal) |
| **Pri** | Gratis (limit) | ‚≠ê Gratis (san limit) |
| **Espas disk** | 0 MB | 2.5GB - 13GB |

### üéØ Pi Bon Tradiksyon

NLLB antrennen espesyalman sou k√≤ t√®ks Krey√≤l Ayisyen, sa ki bay:
- Gram√® pi presi
- Vokabil√® pi rich
- Resp√® pou kont√®ks kiltir√®l
- Mo idyomatik ki pi natir√®l

## Enstalasyon

### 1. Depandans yo

```bash
# Tout depandans yo deja nan requirements.txt
pip install transformers torch sentencepiece langdetect
```

### 2. Verifye enstalasyon

```bash
python -c "import transformers; print('‚úÖ Transformers OK')"
python -c "import torch; print('‚úÖ PyTorch OK')"
```

## Itilizasyon

### Met√≤d 1: Batch File (Pi Senp) ‚≠ê

```bash
# Windows: Double-cliquer sur le fichier
TRADUIRE_NLLB.bat

# Ou pase fichye a k√≤m param√®t
TRADUIRE_NLLB.bat mon_texte.txt

# Ou pase fichye a ak mod√®l
TRADUIRE_NLLB.bat mon_texte.txt medium
```

### Met√≤d 2: Liy K√≤mand

```bash
# Tradiksyon senp
python traduire_nllb.py data/test_document.txt

# Ak mod√®l espesifik
python traduire_nllb.py mon_livre.txt --modele medium

# Ak tay segment kustom
python traduire_nllb.py texte.txt --chunk 300
```

### Met√≤d 3: Nan k√≤d Python

```python
from traduire_nllb import TraducteurNLLB

# Enstansye traducteur a
traducteur = TraducteurNLLB()

# Tradwi t√®ks
texte_francais = "Bonjour, comment allez-vous aujourd'hui?"
texte_kreyol = traducteur.traduire(texte_francais, langue_cible='ht')

print(texte_kreyol)
# ‚Üí "Bonjou, kijan ou ye jodi a?"
```

## Mod√®l yo

### facebook/nllb-200-distilled-600M (Rekomande) ‚≠ê

- **Tay**: ~2.5 GB
- **Vit√®s**: Rapid
- **Kalite**: Tr√® bon
- **Pou**: Majorite ka itilizasyon

```bash
python traduire_nllb.py texte.txt --modele distilled
```

### facebook/nllb-200-1.3B (Pi Bon Kalite)

- **Tay**: ~5 GB
- **Vit√®s**: Mwayen
- **Kalite**: Ekselan
- **Pou**: Travay pwofesyon√®l

```bash
python traduire_nllb.py texte.txt --modele medium
```

### facebook/nllb-200-3.3B (Kalite Sipery√®)

- **Tay**: ~13 GB
- **Vit√®s**: Dousman
- **Kalite**: Eksepsyon√®l
- **Pou**: Pwoj√® enp√≤tan, liv

```bash
python traduire_nllb.py texte.txt --modele large
```

## Opsyon yo

### --modele (Chwazi mod√®l)

```bash
python traduire_nllb.py fichier.txt --modele distilled  # Rapid
python traduire_nllb.py fichier.txt --modele medium     # Mwayen
python traduire_nllb.py fichier.txt --modele large      # Lant
```

### --chunk (Tay segment)

```bash
# Pi piti segment pou memwa limite
python traduire_nllb.py fichier.txt --chunk 200

# Pi gwo segment pou pi bon kont√®ks
python traduire_nllb.py fichier.txt --chunk 800
```

## Egzanp Itilizasyon

### Tradwi yon liv

```bash
# Itilize mod√®l medium pou bon kalite
python traduire_nllb.py livres/mon_roman.txt --modele medium

# Rezilta nan: output/mon_roman_nllb/mon_roman_kreyol_nllb.txt
```

### Tradwi plizy√® fichye

```bash
# Kreye yon script
for %%f in (data\*.txt) do (
    python traduire_nllb.py "%%f"
)
```

### Tradwi dokiman PDF

```bash
# 1. Ekstrak t√®ks PDF a dab√≤
python traduire_document.py mon_document.pdf

# 2. Tradwi ak NLLB
python traduire_nllb.py output/mon_document/mon_document_original.txt
```

## Astuces pou Pi Bon P√®f√≤mans

### 1. Itilize GPU (Si disponib)

```bash
# Verifye si GPU disponib
python -c "import torch; print(f'GPU: {torch.cuda.is_available()}')"

# Si GPU disponib, tradiksyon ap 5-10x pi rapid
```

### 2. Optimize Memwa

```bash
# Redwi tay segment si ou gen memwa limite
python traduire_nllb.py fichier.txt --chunk 200

# Itilize mod√®l pi piti
python traduire_nllb.py fichier.txt --modele distilled
```

### 3. Cache Mod√®l yo

Premye fwa ou itilize, mod√®l la ap telechaje. Apre sa, li rete nan cache:

**Windows**: `C:\Users\<Username>\.cache\huggingface\`
**Linux/Mac**: `~/.cache/huggingface/`

## Konparezon ak Google Translate

### Test: Tradwi 1000 mo

| Metrik | Google | NLLB (distilled) | NLLB (medium) |
|--------|--------|------------------|---------------|
| Tan | 5s | 45s | 90s |
| Kalite | 7/10 | 8.5/10 | 9/10 |
| Idyom | 6/10 | 9/10 | 9.5/10 |
| Gram√® | 7/10 | 9/10 | 9.5/10 |

### Egzanp Konparezon

**T√®ks orijinal** (Fran√ßais):
> "Je voudrais vous pr√©senter mes sinc√®res condol√©ances suite au d√©c√®s de votre m√®re."

**Google Translate**:
> "Mwen ta renmen prezeante ou kondoleances senserite mwen apre lanmo manman ou."

**NLLB**:
> "Mwen ta renmen prezante kondoleyans senserite mwen pou lanm√≤ manman w."

NLLB pi natir√®l e respekte gram√® Krey√≤l Ayisyen.

## Depanaj (Troubleshooting)

### Er√®: "CUDA out of memory"

**Solisyon**:
```bash
# Redwi tay segment
python traduire_nllb.py fichier.txt --chunk 200

# Ou itilize mod√®l pi piti
python traduire_nllb.py fichier.txt --modele distilled
```

### Er√®: "No module named 'transformers'"

**Solisyon**:
```bash
pip install transformers torch sentencepiece
```

### Tradiksyon tr√® lant

**Solisyon**:
1. Itilize mod√®l `distilled` (pi rapid)
2. Verifye si GPU disponib
3. Redwi tay segment

### Kalite tradiksyon pa bon

**Solisyon**:
1. Itilize mod√®l `medium` ou `large`
2. Ogmante tay segment pou pi bon kont√®ks
3. Divize t√®ks a an seksyon lojik

## Lang Sip√≤te

NLLB sip√≤te tradiksyon ant 200+ lang. Pou Krey√≤l:

| Lang | Kod | Kod NLLB |
|------|-----|----------|
| Krey√≤l Ayisyen | `ht` | `hat_Latn` |
| Fran√ßais | `fr` | `fra_Latn` |
| Anglais | `en` | `eng_Latn` |
| Espagnol | `es` | `spa_Latn` |
| Portugais | `pt` | `por_Latn` |

## Resous Adisyon√®l

### Dokimantasyon Ofisy√®l

- [NLLB Model Card](https://huggingface.co/facebook/nllb-200-distilled-600M)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)

### Papye Rech√®ch

- [No Language Left Behind](https://ai.meta.com/research/no-language-left-behind/)

### Sip√≤

- GitHub Issues: [Pwoj√® Krey√≤l IA]
- Email: [ou ka kontakte nou]

## FAQ

### 1. √àske mwen bezwen Ent√®n√®t?

**Non** (apre premye telechajman). Premye fwa, mod√®l la ap telechaje (~2.5GB). Apre sa, tout fonksyone san Ent√®n√®t.

### 2. √àske li fonksyone sou machin san GPU?

**Wi**, men li ap pi lant. GPU rek√≤mande pou t√®ks long.

### 3. Ki mod√®l ki pi bon pou mwen?

- **Eseye rapid**: `distilled`
- **Travay n√≤mal**: `distilled` ou `medium`
- **Pwoj√® pwofesyon√®l**: `medium` ou `large`

### 4. √àske li pi bon pase Google Translate?

**Wi** pou Krey√≤l Ayisyen. NLLB antrennen espesyalman sou Krey√≤l, sa ki bay pi bon kalite.

### 5. Konbyen tan sa pran?

- **distilled**: ~1 minit pou 1000 mo (CPU), ~10s (GPU)
- **medium**: ~2 minit pou 1000 mo (CPU), ~20s (GPU)
- **large**: ~5 minit pou 1000 mo (CPU), ~30s (GPU)

### 6. √àske mwen ka itilize l k√≤m√®syalman?

**Wi**, NLLB se open source ak lisans MIT/Apache 2.0.

## Kontribye

Pou amelyore tradiksyon NLLB pou Krey√≤l:

1. **Rap√≤te pwobl√®m** - Si ou w√® tradiksyon ki pa bon
2. **Pataje egzanp** - Bay bon ak move egzanp tradiksyon
3. **Kontribye k√≤d** - Ede amelyore script la

## Lisans

NLLB se yon pwoj√® Meta Research ak lisans CC-BY-NC 4.0.

---

## üéâ K√≤manse Kounye a!

```bash
# Rapide e senp
TRADUIRE_NLLB.bat mon_texte.txt

# Ou
python traduire_nllb.py mon_texte.txt
```

**Bon tradiksyon!** üá≠üáπ

